{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition\n",
    "import pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import speech_recognition\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening....\n",
      " how have you been feeling since your last visit?\n",
      "Listening....\n",
      " i've been experiencing headaches and dizziness for the past\n",
      "Listening....\n",
      " i said, 10 minutes before that, i saw that scene.\n",
      "Listening....\n",
      " other symptoms.\n",
      "Listening....\n",
      " no, just those.\n",
      "Listening....\n",
      " okay, so i will prescribe medications for you.\n",
      "Listening....\n",
      " and a follow-up.\n",
      "Listening....\n",
      " end recording.\n",
      "Listening....\n",
      "Stopping recording...\n",
      "\n",
      "Final recorded text: how have you been feeling since your last visit?  i've been experiencing headaches and dizziness for the past  i said, 10 minutes before that, i saw that scene.  other symptoms.  no, just those.  okay, so i will prescribe medications for you.  and a follow-up.  end recording.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def record_speech():\n",
    "    recognizer = speech_recognition.Recognizer()\n",
    "    final = \"\"\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            with speech_recognition.Microphone() as mic:\n",
    "                recognizer.adjust_for_ambient_noise(mic, duration=0.2)\n",
    "                print(\"Listening....\")\n",
    "                audio = recognizer.listen(mic)\n",
    "                text = recognizer.recognize_groq(audio)\n",
    "                text = text.lower()\n",
    "                \n",
    "                if \"stop recording\" in text:\n",
    "                    print(\"Stopping recording...\")\n",
    "                    break\n",
    "                    \n",
    "                final += \" \" + text\n",
    "                print(text)\n",
    "                \n",
    "        except speech_recognition.UnknownValueError:\n",
    "            print(\"End. Stopping recording...\")\n",
    "            break  # Stop instead of continue\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            break  \n",
    "    \n",
    "    return final.strip()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    recorded_text = record_speech()\n",
    "    print(\"\\nFinal recorded text:\", recorded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=\"How have you been feeling since your last visit. I've been experiencing headaches and dizziness for the past week.Any other symptoms. No just those. Okay so I'll be prescribing ibuprofen 400 mg. Follow up in 2 weeks if symptoms persist.What is the issue?. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    groq_api_key=\"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\",\n",
    "    model=\"llama-3.3-70b-versatile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the labeled conversation:\n",
      "\n",
      "Doctor: How have you been feeling since your last visit.\n",
      "Patient: I've been experiencing headaches and dizziness for the past week.\n",
      "Doctor: Any other symptoms.\n",
      "Patient: No just those.\n",
      "Doctor: Okay so I'll be prescribing ibuprofen 400 mg. Follow up in 2 weeks if symptoms persist.\n",
      "Patient: What is the issue?.\n"
     ]
    }
   ],
   "source": [
    "instructions=\"\"\"\n",
    "You will be provided with a conversation between a patient and doctor.\n",
    "Your job is to label each conversation as doctor or patient.\n",
    "Carefully analyze each sentence.\n",
    "\n",
    "For example:\n",
    "Input: How have you been feeling today\n",
    "Output: Doctor: How have you been feeling today\n",
    "\n",
    "The conversation is as follows: {conversation}\n",
    "\"\"\"\n",
    "sys=instructions.format(conversation=final)\n",
    "result=llm.invoke(sys).content\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjective: \n",
      "- The patient has been experiencing headaches and dizziness for the past week.\n",
      "\n",
      "Objective: \n",
      "- No other symptoms mentioned.\n",
      "\n",
      "Assessment: \n",
      "- Information not provided.\n",
      "\n",
      "Plan: \n",
      "- Prescribing ibuprofen 400 mg, follow up in 2 weeks if symptoms persist.\n"
     ]
    }
   ],
   "source": [
    "instructions=\"\"\"\n",
    "You will be provided with  a conversation between a patient and a doctor.\n",
    "Your job is to carefully analyse the coversation and summarize it into a SOAP format.\n",
    "\n",
    "The SOAP format is as follows along with an example:\n",
    "\n",
    "Subjective:  \n",
    "- What has the patient been feeling \n",
    "\n",
    "Objective:  \n",
    "- Any other symptoms\n",
    "\n",
    "Assessment:  \n",
    "- Your Assesment\n",
    "\n",
    "Plan:  \n",
    "- What medicines or procedures to take.\n",
    "\n",
    "Output only in this format and nothing else. Do not cook up any information.\n",
    "Remember, if any information is missing do not add your own information just say information not provided.\n",
    "For example if assesment is not provided say information not provided.\n",
    "\n",
    "The conversation is as follows: {conversation}\n",
    "\"\"\"\n",
    "sys=instructions.format(conversation=result)\n",
    "result=llm.invoke(sys)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subjective: \\n- The patient has been experiencing headaches and dizziness for the past week.\\n\\nObjective: \\n- No other symptoms mentioned.\\n\\nAssessment: \\n- Information not provided.\\n\\nPlan: \\n- Prescribing ibuprofen 400 mg, follow up in 2 weeks if symptoms persist.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hey doctor you have not included the assessment.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 190, 'total_tokens': 200, 'completion_time': 0.036363636, 'prompt_time': 0.026803192, 'queue_time': 1.0269885189999999, 'total_time': 0.063166828}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_fcc3b74982', 'finish_reason': 'stop', 'logprobs': None} id='run-bae4ade1-9b68-4eea-8681-a21e5e9dfde1-0' usage_metadata={'input_tokens': 190, 'output_tokens': 10, 'total_tokens': 200}\n"
     ]
    }
   ],
   "source": [
    "instructions=\"\"\"\n",
    "    You will be given the SOAP summary of a conversation between a doctor and patient.\n",
    "    You will find out under what heading is information not provided.\n",
    "    Using that heading create a question to prompt the doctor that he/she has not provided the specific information.\n",
    "    Just give me the question nothing else. \n",
    "    Remember clearly!!! Do not add anything else.\n",
    "    For example:\n",
    "    Hey doctor you have not included the plan. \n",
    "\n",
    "    The summary is as follows: {summary}  \n",
    "    \"\"\"\n",
    "sys=instructions.format(summary=result.content)\n",
    "result=llm.invoke(sys)\n",
    "engine=pyttsx3.init()\n",
    "engine.say(result.content)\n",
    "engine.runAndWait()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine=pyttsx3.init()\n",
    "answer=\"Hello\"\n",
    "engine.say(answer)\n",
    "engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition\n",
    "import pyttsx3\n",
    "from model import llm\n",
    "from classes import MessageState\n",
    "from langchain_core.messages import AIMessage,SystemMessage,HumanMessage\n",
    "from langgraph.graph import END\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "engine=pyttsx3.init()\n",
    "recognizer=speech_recognition.Recognizer()\n",
    "\n",
    "@tool\n",
    "def record_speech(state: MessageState):\n",
    "    \"\"\"\n",
    "    This tool is called whenever the user asks to take down notes in any form.\n",
    "    \n",
    "    \"\"\"\n",
    "    engine.say(\"Hey I am ready to take notes!!!\")\n",
    "    engine.runAndWait()\n",
    "\n",
    "    final = \"\"\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            with speech_recognition.Microphone() as mic:\n",
    "                recognizer.adjust_for_ambient_noise(mic, duration=0.2)\n",
    "                print(\"Listening....\")\n",
    "                audio = recognizer.listen(mic)\n",
    "                text = recognizer.recognize_google(audio)\n",
    "                text = text.lower()\n",
    "                \n",
    "                if \"end\" in text:\n",
    "                    print(\"Stopping recording...\")\n",
    "                    break\n",
    "                    \n",
    "                final += \" \" + text\n",
    "                print(text)\n",
    "                \n",
    "        except speech_recognition.UnknownValueError:\n",
    "            print(\"End. Stopping recording...\")\n",
    "            break \n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            break \n",
    "    \n",
    "    return {\"recorded_text\": final.strip()}\n",
    "\n",
    "def format_conversation(state: MessageState):\n",
    "    recorded_text=state[\"recorded_text\"]\n",
    "    instructions=\"\"\"\n",
    "    You will be provided with a conversation between a patient and doctor.\n",
    "    Your job is to label each conversation as doctor or patient.\n",
    "    Carefully analyze each sentence.\n",
    "\n",
    "    For example:\n",
    "    Input: How have you been feeling today\n",
    "    Output: Doctor: How have you been feeling today\n",
    "\n",
    "    The conversation is as follows: {conversation}\n",
    "    \"\"\"\n",
    "    sys=instructions.format(conversation=recorded_text)\n",
    "    result=llm.invoke([AIMessage(content=sys)]).content\n",
    "    return {\"messages\":result}\n",
    "\n",
    "def SOAP_formatter(state: MessageState):\n",
    "    conversation=state[\"messages\"][-1].content\n",
    "    instructions=\"\"\"\n",
    "    You will be provided with  a conversation between a patient and a doctor.\n",
    "    Your job is to carefully analyse the coversation and summarize it into a SOAP format.\n",
    "\n",
    "    The SOAP format is as follows along with an example:\n",
    "\n",
    "    Subjective:  \n",
    "    - What has the patient been feeling \n",
    "\n",
    "    Objective:  \n",
    "    - Any other symptoms\n",
    "\n",
    "    Assessment:  \n",
    "    - Your Assesment\n",
    "\n",
    "    Plan:  \n",
    "    - What medicines or procedures to take.\n",
    "\n",
    "    Output only in this format and nothing else. Do not make up any information if any information is missing dont add your own information just say information not provided.\n",
    "    For example if assesment is not provided say information not provided.\n",
    "\n",
    "    The conversation is as follows: {conversation}\n",
    "    \"\"\"\n",
    "    sys=instructions.format(conversation=conversation)\n",
    "    result=llm.invoke([AIMessage(content=sys)])\n",
    "    return {\"messages\":result.content}\n",
    "\n",
    "\n",
    "\n",
    "def router_condition(state: MessageState):\n",
    "    soap=state[\"messages\"][-1]\n",
    "    if \"information not provided\" in soap.content.lower():\n",
    "        return \"reask_node\"\n",
    "    else:\n",
    "        return END\n",
    "\n",
    "def reask(state: MessageState):\n",
    "    soap=state[\"messages\"][-1].content\n",
    "    instructions=\"\"\"\n",
    "    You will be given the SOAP summary of a conversation between a doctor and patient.\n",
    "    You will find out under what heading is information not provided.\n",
    "    Using that heading create a question to prompt the doctor that he/she has not provided the specific information.\n",
    "    Just give me the question nothing else. \n",
    "    Remember clearly!!! Do not add anything else.\n",
    "    For example:\n",
    "    Hey doctor you have not included the plan. \n",
    "\n",
    "    The summary is as follows: {summary} \n",
    "    \"\"\"\n",
    "    sys=instructions.format(summary=soap)\n",
    "    result=llm.invoke([AIMessage(content=sys)])\n",
    "    engine.say(result.content)\n",
    "    engine.runAndWait()\n",
    "    return {\"reask_doctor\":result.content}\n",
    "\n",
    "@tool\n",
    "def multiply(a: int,b: int):\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "    a: first int\n",
    "    b: second int\n",
    "    \"\"\"\n",
    "    return a*b\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools=llm.bind_tools(tools=[multiply,record_speech])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_call=llm_with_tools.invoke(\"Take down notes for me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'record_speech'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call.additional_kwargs['tool_calls'][0]['function']['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'NoneType' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwhisper\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyaudio\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwave\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shawn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper.py:69\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m CAN_FALLOCATE:\n\u001b[0;32m     68\u001b[0m   libc_name \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mfind_library(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m   libc \u001b[38;5;241m=\u001b[39m \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlibc_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m   c_off64_t \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int64\n\u001b[0;32m     71\u001b[0m   c_off_t \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int\n",
      "File \u001b[1;32mc:\\Users\\shawn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ctypes\\__init__.py:369\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnt\u001b[39;00m\n\u001b[0;32m    368\u001b[0m mode \u001b[38;5;241m=\u001b[39m nt\u001b[38;5;241m.\u001b[39m_LOAD_LIBRARY_SEARCH_DEFAULT_DIRS\n\u001b[1;32m--> 369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name:\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m nt\u001b[38;5;241m.\u001b[39m_getfullpathname(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name)\n\u001b[0;32m    371\u001b[0m     mode \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m nt\u001b[38;5;241m.\u001b[39m_LOAD_LIBRARY_SEARCH_DLL_LOAD_DIR\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'NoneType' is not iterable"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "# Load the Whisper model\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Setup microphone input\n",
    "chunk = 1024  # Buffer size\n",
    "format = pyaudio.paInt16\n",
    "channels = 1\n",
    "rate = 44100\n",
    "record_seconds = 5  # Adjust duration to your needs\n",
    "\n",
    "# Initialize PyAudio\n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "# Open the microphone stream\n",
    "stream = audio.open(format=format, channels=channels,\n",
    "                    rate=rate, input=True,\n",
    "                    frames_per_buffer=chunk)\n",
    "\n",
    "print(\"Listening...\")\n",
    "\n",
    "frames = []\n",
    "for i in range(0, int(rate / chunk * record_seconds)):\n",
    "    data = stream.read(chunk)\n",
    "    frames.append(data)\n",
    "\n",
    "print(\"Recording finished.\")\n",
    "\n",
    "# Stop and close the stream\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()\n",
    "\n",
    "# Save the audio to a file\n",
    "wf = wave.open(\"output.wav\", \"wb\")\n",
    "wf.setnchannels(channels)\n",
    "wf.setsampwidth(audio.get_sample_size(format))\n",
    "wf.setframerate(rate)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()\n",
    "\n",
    "# Transcribe the audio\n",
    "result = model.transcribe(\"output.wav\")\n",
    "print(\"Transcription: \", result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': 0, 'structVersion': 2, 'name': 'Microsoft Sound Mapper - Input', 'hostApi': 0, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "{'index': 1, 'structVersion': 2, 'name': 'Microphone Array (AMD Audio Dev', 'hostApi': 0, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "{'index': 2, 'structVersion': 2, 'name': 'Microsoft Sound Mapper - Output', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "{'index': 3, 'structVersion': 2, 'name': 'Speakers (Realtek(R) Audio)', 'hostApi': 0, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.09, 'defaultLowOutputLatency': 0.09, 'defaultHighInputLatency': 0.18, 'defaultHighOutputLatency': 0.18, 'defaultSampleRate': 44100.0}\n",
      "{'index': 4, 'structVersion': 2, 'name': 'Primary Sound Capture Driver', 'hostApi': 1, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.12, 'defaultLowOutputLatency': 0.0, 'defaultHighInputLatency': 0.24, 'defaultHighOutputLatency': 0.0, 'defaultSampleRate': 44100.0}\n",
      "{'index': 5, 'structVersion': 2, 'name': 'Microphone Array (AMD Audio Device)', 'hostApi': 1, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.12, 'defaultLowOutputLatency': 0.0, 'defaultHighInputLatency': 0.24, 'defaultHighOutputLatency': 0.0, 'defaultSampleRate': 44100.0}\n",
      "{'index': 6, 'structVersion': 2, 'name': 'Primary Sound Driver', 'hostApi': 1, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.0, 'defaultLowOutputLatency': 0.12, 'defaultHighInputLatency': 0.0, 'defaultHighOutputLatency': 0.24, 'defaultSampleRate': 44100.0}\n",
      "{'index': 7, 'structVersion': 2, 'name': 'Speakers (Realtek(R) Audio)', 'hostApi': 1, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.0, 'defaultLowOutputLatency': 0.12, 'defaultHighInputLatency': 0.0, 'defaultHighOutputLatency': 0.24, 'defaultSampleRate': 44100.0}\n",
      "{'index': 8, 'structVersion': 2, 'name': 'Speakers (Realtek(R) Audio)', 'hostApi': 2, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.0, 'defaultLowOutputLatency': 0.003, 'defaultHighInputLatency': 0.0, 'defaultHighOutputLatency': 0.01, 'defaultSampleRate': 48000.0}\n",
      "{'index': 9, 'structVersion': 2, 'name': 'Microphone Array (AMD Audio Device)', 'hostApi': 2, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.003, 'defaultLowOutputLatency': 0.0, 'defaultHighInputLatency': 0.01, 'defaultHighOutputLatency': 0.0, 'defaultSampleRate': 48000.0}\n",
      "{'index': 10, 'structVersion': 2, 'name': 'Headphones 1 (Realtek HD Audio 2nd output with HAP)', 'hostApi': 3, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 48000.0}\n",
      "{'index': 11, 'structVersion': 2, 'name': 'Headphones 2 (Realtek HD Audio 2nd output with HAP)', 'hostApi': 3, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 44100.0}\n",
      "{'index': 12, 'structVersion': 2, 'name': 'PC Speaker (Realtek HD Audio 2nd output with HAP)', 'hostApi': 3, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 48000.0}\n",
      "{'index': 13, 'structVersion': 2, 'name': 'Speakers 1 (Realtek HD Audio output with HAP)', 'hostApi': 3, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 48000.0}\n",
      "{'index': 14, 'structVersion': 2, 'name': 'Speakers 2 (Realtek HD Audio output with HAP)', 'hostApi': 3, 'maxInputChannels': 0, 'maxOutputChannels': 2, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 44100.0}\n",
      "{'index': 15, 'structVersion': 2, 'name': 'PC Speaker (Realtek HD Audio output with HAP)', 'hostApi': 3, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 48000.0}\n",
      "{'index': 16, 'structVersion': 2, 'name': 'Stereo Mix (Realtek HD Audio Stereo input)', 'hostApi': 3, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 48000.0}\n",
      "{'index': 17, 'structVersion': 2, 'name': 'Microphone (Realtek HD Audio Mic input)', 'hostApi': 3, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 44100.0}\n",
      "{'index': 18, 'structVersion': 2, 'name': 'Microphone Array (AMDAfdInstall Wave Microphone - 1)', 'hostApi': 3, 'maxInputChannels': 2, 'maxOutputChannels': 0, 'defaultLowInputLatency': 0.01, 'defaultLowOutputLatency': 0.01, 'defaultHighInputLatency': 0.04, 'defaultHighOutputLatency': 0.04, 'defaultSampleRate': 48000.0}\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "\n",
    "p=pyaudio.PyAudio()\n",
    "for i in range(p.get_device_count()):\n",
    "    print(p.get_device_info_by_index(i))\n",
    "\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vosk import Model, KaldiRecognizer\n",
    "import pyaudio\n",
    "\n",
    "# Load the Vosk model\n",
    "model = Model(model_name=\"vosk-model-small-en-us-0.15\")\n",
    "recognizer = KaldiRecognizer(model, 16000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'success': True, 'error': None, 'transcription': '400'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "r = sr.Recognizer()\n",
    "m = sr.Microphone()\n",
    "recognize_speech_from_mic(r, m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import websockets\n",
    "import asyncio\n",
    "import base64\n",
    "import json\n",
    "auth_key=\"0c3d3ebe5422439599668cd1f2e7fa2c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAMER_PER_BUFFER=3200\n",
    "FORMAT=pyaudio.paInt16\n",
    "CHANNELS=1\n",
    "RATE=16000\n",
    "p=pyaudio.PyAudio()\n",
    "\n",
    "stream=p.open(format=FORMAT,channels=CHANNELS,rate=RATE,input=True,frames_per_buffer=FRAMER_PER_BUFFER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'groq.api'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgroq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgroq\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msounddevice\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'groq.api'"
     ]
    }
   ],
   "source": [
    "import groq.api as groq\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "\n",
    "# Define Groq Cloud settings\n",
    "GROQ_API_KEY = \"your_groq_api_key_here\"\n",
    "GROQ_MODEL_NAME = \"whisper\"\n",
    "\n",
    "# Initialize Groq API\n",
    "groq.init(api_key=GROQ_API_KEY)\n",
    "\n",
    "# Load the Whisper model on Groq Cloud\n",
    "model = groq.load_model(GROQ_MODEL_NAME)\n",
    "\n",
    "# Function to process audio data\n",
    "def process_audio_data(data):\n",
    "    # Convert raw audio to numpy array\n",
    "    audio = np.frombuffer(data, dtype=np.int16)\n",
    "    return audio\n",
    "\n",
    "# Function to handle real-time transcription\n",
    "def transcribe_audio(indata, frames, time, status):\n",
    "    audio_data = process_audio_data(indata)\n",
    "    transcription = model.transcribe(audio_data)\n",
    "    print(\"Transcription:\", transcription['text'])\n",
    "\n",
    "# Start the real-time audio stream\n",
    "with sd.InputStream(callback=transcribe_audio, channels=1, samplerate=16000):\n",
    "    print(\"Listening... Press Ctrl+C to stop.\")\n",
    "    sd.sleep(100000)  # Keep the stream alive for a long time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "soap=\"\"\"Subjective:  \n",
    "- The patient has been experiencing headaches and dizziness for the past week.  \n",
    "Objective:  \n",
    "-Information missing\n",
    "Assessment:  \n",
    "- Patient has migraine\n",
    "Plan:  \n",
    "- Information missing\"\"\"\n",
    "\n",
    "instructions=\"\"\"\n",
    "You will be given the SOAP summary which has the following headings: (Subjective, Objective, Assessment and Plan ).\n",
    "Identify which SOAP heading or headings lacks information.\n",
    "Create a targeted question to prompt the doctor about the missing information.\n",
    "Respond ONLY with the question, starting with \"Hey doctor\". \n",
    "\n",
    "Example output:\n",
    "Hey doctor you have not included the Assesment.\n",
    "\n",
    "### Remember do not add your own information just ask a question!!!\n",
    "The SOAP summary is as follows: \n",
    "{summary}  \n",
    "\"\"\"\n",
    "sys=instructions.format(summary=soap)\n",
    "result=llm.invoke([AIMessage(content=sys)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey doctor you have not included the Objective and Plan.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Output: \n",
      "    Doctor: Start taking notes. \n",
      "    Patient: I have fever. \n",
      "    Doctor: Anything else. \n",
      "    Patient: No. \n",
      "    Doctor: Take rest\n"
     ]
    }
   ],
   "source": [
    "recorded_text=\"Start taking notes. I have fever. Anything else. No. Take rest \"\n",
    "\n",
    "instructions=\"\"\"\n",
    "    You will be provided with a conversation between a patient and doctor.\n",
    "    Your job is to label each conversation as doctor or patient.\n",
    "    Remember, do not make up or put your own information in this.\n",
    "    Carefully analyze each sentence.\n",
    "\n",
    "    For example:\n",
    "    Input: How have you been feeling today\n",
    "    Output: Doctor: How have you been feeling today\n",
    "\n",
    "    Remember..do not output any sentence which is not a part of the conversation. Output only the conversation.\n",
    "    The conversation is as follows: {conversation}\n",
    "    \"\"\"\n",
    "sys=instructions.format(conversation=recorded_text)\n",
    "result=llm.invoke([AIMessage(content=sys)]).content\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n    Subjective: \\n    - The patient has been feeling feverish\\n\\n    Objective: \\n    - No other symptoms\\n\\n    Assessment: \\n    - Information missing\\n\\n    Plan: \\n    - Take rest and take paracetamol'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instructions=\"\"\"\n",
    "    You will be provided with  a conversation between a patient and a doctor.\n",
    "    Your job is to carefully analyse the coversation and summarize it into a SOAP format.\n",
    "    Do not include and doctor patient conversation, just the SOAP format.\n",
    "\n",
    "    The SOAP format is as follows along with an example:\n",
    "\n",
    "    Subjective:  \n",
    "    - What has the patient been feeling \n",
    "\n",
    "    Objective:  \n",
    "    - Any other symptoms\n",
    "\n",
    "    Assessment:  \n",
    "    - The doctor's Assesment \n",
    "\n",
    "    Plan:  \n",
    "    - What medicines or procedures to take. (Do not add your own information)\n",
    "\n",
    "    Output only in this format and nothing else. Do not make up any information if any information is missing do not add your own information just say information missing.\n",
    "    For example if assesment is not provided say information missing.\n",
    "\n",
    "    The conversation is as follows: {conversation}\n",
    "    \"\"\"\n",
    "sys=instructions.format(conversation=result)\n",
    "result=llm.invoke([AIMessage(content=sys)])\n",
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
